#!/usr/bin/env python3
"""
SE Evidence Finder (COPY MODE)
Copies (does not move) and normalizes social-engineering evidence into a Reports/<campaign>/Evidence/ folder.

Usage:
    python se_evidence_finder_copy.py --src /path/to/raw_evidence --campaign CAMPAIGN_NAME --vault /path/to/vault

What it does:
- Scans src directory recursively for common evidence files (images, pcap, txt, eml, csv, mp3).
- Normalizes filenames to: YYYYMMDD_HHMMSS_<type>_<shorthash>.<ext>
- Copies files into vault/Reports/<campaign>/Evidence/{images,pcap,logs,emails,attachments,audio,other}
- Generates a manifest JSON and an index Markdown (Evidence_Index.md) summarizing files and metadata
- Safe: files are copied (originals preserved). Use when you want to keep originals intact.

Requires: Python 3.8+
"""
import argparse
import hashlib
import json
import logging
import os
import shutil
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Optional

# file type mapping
TYPE_MAP = {
    'images': ['.png', '.jpg', '.jpeg', '.gif', '.bmp', '.tiff', '.webp'],
    'pcap': ['.pcap', '.pcapng'],
    'logs': ['.log', '.txt', '.csv'],
    'emails': ['.eml', '.msg'],
    'audio': ['.mp3', '.wav', '.m4a'],
    'attachments': ['.pdf', '.docx', '.xlsx', '.pptx', '.zip', '.rar', '.7z'],
}

EXT_SET = {ext for lst in TYPE_MAP.values() for ext in lst}


def short_hash(path, length=8):
    h = hashlib.sha256()
    with open(path, 'rb') as f:
        while True:
            chunk = f.read(8192)
            if not chunk:
                break
            h.update(chunk)
    return h.hexdigest()[:length]


def detect_type(ext):
    ext = ext.lower()
    for k, v in TYPE_MAP.items():
        if ext in v:
            return k
    return 'other'


def ensure_dir(path):
    os.makedirs(path, exist_ok=True)


def find_existing_by_hash(dest_dir: str, short_h: str, ext: str):
    """Return an existing file path in dest_dir that ends with _<short_h><ext>, if any."""
    try:
        for name in os.listdir(dest_dir):
            if name.endswith(f"_{short_h}{ext}"):
                return os.path.join(dest_dir, name)
    except FileNotFoundError:
        return None
    return None


def setup_logging(log_file: Optional[str]):
    logger = logging.getLogger("evidence_finder")
    logger.setLevel(logging.INFO)
    # clear existing handlers (for repeated invocations)
    logger.handlers = []

    fmt = logging.Formatter("%(asctime)s [%(levelname)s] %(message)s")
    ch = logging.StreamHandler()
    ch.setLevel(logging.INFO)
    ch.setFormatter(fmt)
    logger.addHandler(ch)

    if log_file:
        # ensure parent dir exists
        parent = os.path.dirname(os.path.abspath(log_file))
        if parent:
            os.makedirs(parent, exist_ok=True)
        fh = logging.FileHandler(log_file, encoding="utf-8")
        fh.setLevel(logging.INFO)
        fh.setFormatter(fmt)
        logger.addHandler(fh)

    return logger


def normalize_and_copy(src_path: str, evidence_root: str, mode: str = "copy"):
    stat = os.stat(src_path)
    mtime = datetime.fromtimestamp(stat.st_mtime).strftime('%Y%m%d_%H%M%S')
    ext = os.path.splitext(src_path)[1].lower()
    ftype = detect_type(ext)
    h = short_hash(src_path)
    filename = f"{mtime}_{ftype}_{h}{ext}"
    dest_dir = os.path.join(evidence_root, ftype)
    ensure_dir(dest_dir)

    # dedupe by short hash if file already archived
    existing = find_existing_by_hash(dest_dir, h, ext)
    dedup = False
    if existing:
        dest_path = existing
        dedup = True
    else:
        dest_path = os.path.join(dest_dir, filename)
        if os.path.exists(dest_path):
            base, ext2 = os.path.splitext(filename)
            dest_path = os.path.join(dest_dir, f"{base}_dup{ext2}")
        if mode == "move":
            shutil.move(src_path, dest_path)
        else:
            shutil.copy2(src_path, dest_path)

    return {
        'original_path': src_path,
        'dest_path': dest_path,
        'type': ftype,
        'filename': os.path.basename(dest_path),
        'mtime': mtime,
        'size_bytes': os.path.getsize(dest_path),
        'sha256_short': h,
        'dedup': dedup,
        'mode': mode,
    }


def scan_and_archive(src_root: str, vault_root: str, campaign: str,
                     include_other: bool = False,
                     max_size_mb: int = 0,
                     workers: int = 0,
                     dry_run: bool = False,
                     mode: str = "copy",
                     log_file: Optional[str] = None):
    logger = setup_logging(log_file)
    scanned_at = datetime.utcnow().strftime('%Y-%m-%dT%H:%M:%SZ')
    manifest = {
        'campaign': campaign,
        'scanned_at': scanned_at,
        'files': [],
        'stats': {},
        'mode': mode,
    }

    max_bytes = max_size_mb * 1024 * 1024 if max_size_mb and max_size_mb > 0 else 0

    def should_include(path: str) -> bool:
        ext = os.path.splitext(path)[1].lower()
        if ext not in EXT_SET and not include_other:
            return False
        if max_bytes and os.path.exists(path):
            try:
                if os.path.getsize(path) > max_bytes:
                    return False
            except OSError:
                return False
        return True

    candidates = []
    total_seen = 0
    for root, _dirs, files in os.walk(src_root):
        for fname in files:
            total_seen += 1
            path = os.path.join(root, fname)
            if should_include(path):
                candidates.append(path)

    if dry_run:
        for p in candidates:
            ext = os.path.splitext(p)[1].lower()
            ftype = detect_type(ext)
            logger.info(f"[DRY] {p} -> {ftype}")
        logger.info(f"[DRY] {len(candidates)} candidates out of {total_seen} scanned")
        return None, None

    reports_root = os.path.join(vault_root, 'Reports', campaign)
    evidence_root = os.path.join(reports_root, 'Evidence')
    ensure_dir(evidence_root)

    copied = 0
    deduped = 0
    errors = 0
    moved = 0

    max_workers = workers if workers and workers > 0 else min(32, (os.cpu_count() or 2) * 2)
    with ThreadPoolExecutor(max_workers=max_workers) as pool:
        futures = [pool.submit(normalize_and_copy, p, evidence_root, mode) for p in candidates]
        for fut in as_completed(futures):
            try:
                meta = fut.result()
                manifest['files'].append(meta)
                if meta.get('dedup'):
                    deduped += 1
                    logger.info(f"Dedup: {meta['filename']} -> {meta['dest_path']}")
                else:
                    if mode == 'move':
                        moved += 1
                        logger.info(f"Moved: {meta['filename']} -> {meta['dest_path']}")
                    else:
                        copied += 1
                        logger.info(f"Copied: {meta['filename']} -> {meta['dest_path']}")
            except Exception as e:
                errors += 1
                logger.error(f"Error: {e}")

    # write manifest and markdown index
    ensure_dir(reports_root)
    manifest['stats'] = {
        'total_scanned': total_seen,
        'candidates': len(candidates),
        'copied': copied,
        'moved': moved,
        'deduped': deduped,
        'errors': errors,
        'workers': max_workers,
    }

    manifest_path = os.path.join(reports_root, 'evidence_manifest.json')
    with open(manifest_path, 'w', encoding='utf-8') as f:
        json.dump(manifest, f, indent=2)

    # create Evidence_Index.md
    md_lines = [
        f"# Evidence Index - {campaign}",
        "",
        f"Generated: {scanned_at}",
        "",
        "| Filename | Type | Size (bytes) | MTime | SHA256_short | Path |",
        "|---|---:|---:|---|---|---|"
    ]
    for fmeta in manifest['files']:
        md_lines.append(
            f"| {fmeta['filename']} | {fmeta['type']} | {fmeta['size_bytes']} | {fmeta['mtime']} | {fmeta['sha256_short']} | {fmeta['dest_path']} |"
        )
    md_path = os.path.join(reports_root, 'Evidence_Index.md')
    with open(md_path, 'w', encoding='utf-8') as f:
        f.write('\n'.join(md_lines))
    logger.info(f"Manifest written to {manifest_path}")
    logger.info(f"Markdown index written to {md_path}")
    return manifest_path, md_path


def main():
    parser = argparse.ArgumentParser(
        description=(
            "SE Evidence Finder (COPY MODE) â€” copy and normalize evidence into "
            "vault Reports/<campaign>/Evidence/"
        )
    )
    parser.add_argument("--src", required=True, help="Source folder containing raw evidence")
    parser.add_argument("--campaign", required=True, help="Campaign name (used as Reports/<campaign>)")
    parser.add_argument("--vault", required=True, help="Path to your vault root (where Reports/ will be created)")
    parser.add_argument("--include-other", action="store_true", help="Also include files with unknown extensions (goes to 'other')")
    parser.add_argument("--max-size-mb", type=int, default=0, help="Skip files larger than this size (0 = no limit)")
    parser.add_argument("--workers", type=int, default=0, help="Number of worker threads (0 = auto)")
    parser.add_argument("--dry-run", action="store_true", help="List actions without writing any files")
    parser.add_argument("--mode", choices=["copy", "move"], default="copy", help="Operation mode: copy (default) or move")
    parser.add_argument("--log-file", default=None, help="Optional log file path to write detailed logs")
    args = parser.parse_args()

    if not os.path.isdir(args.src):
        raise SystemExit(f"Source folder not found: {args.src}")
    if not os.path.isdir(args.vault):
        raise SystemExit(f"Vault folder not found: {args.vault}")

    logger = setup_logging(args.log_file)
    logger.info(
        f"Scanning {args.src} -> campaign {args.campaign} in vault {args.vault} (mode={args.mode})"
        + (" [DRY-RUN]" if args.dry_run else "")
    )
    scan_and_archive(
        args.src,
        args.vault,
        args.campaign,
        include_other=args.include_other,
        max_size_mb=args.max_size_mb,
        workers=args.workers,
        dry_run=args.dry_run,
        mode=args.mode,
        log_file=args.log_file,
    )


if __name__ == '__main__':
    main()
